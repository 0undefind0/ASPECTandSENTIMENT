{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature     \t| Type \t| Description                                                       \t|\n",
    "|:-------------\t|:------\t|:-------------------------------------------------------------------\t|\n",
    "| content     \t| obj  \t| Raw text containing user reviews                                  \t|\n",
    "| content_stem \t| obj  \t| Pre-processed text for modeling                                   \t|\n",
    "| score       \t| int  \t| No. of star ratings the user gave (1-5)                           \t|\n",
    "| target      \t| int  \t| Target variable <br>Postive sentiment: 0<br>Negative sentiment: 1 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Libraries for classical machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Libraries for deep learning\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Embedding, Dropout, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Libraries for topic modeling\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import spacy\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Shopee's Google Play reviews that we have collected, cleaned and pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the clean dataset\n",
    "reviews = pd.read_csv('../SentimentModel/clean_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_stem</th>\n",
       "      <th>score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Highly Recommended The only shopping app that ...</td>\n",
       "      <td>highli recommend shop love shop reliabl custom...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In my experiences, Shopee offers better price ...</td>\n",
       "      <td>experi offer better price product reason unkno...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So far so good, but the delivery can be quite ...</td>\n",
       "      <td>far good deliveri quit slow time even though l...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facing really bad experience with Shopee wareh...</td>\n",
       "      <td>face realli experi warehous shown return track...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edit to 3 star: Shopee finally refunded my ite...</td>\n",
       "      <td>edit star final refund return declar miss cour...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   \n",
       "0  Highly Recommended The only shopping app that ...  \\\n",
       "1  In my experiences, Shopee offers better price ...   \n",
       "2  So far so good, but the delivery can be quite ...   \n",
       "3  Facing really bad experience with Shopee wareh...   \n",
       "4  Edit to 3 star: Shopee finally refunded my ite...   \n",
       "\n",
       "                                        content_stem  score  target  \n",
       "0  highli recommend shop love shop reliabl custom...      5       0  \n",
       "1  experi offer better price product reason unkno...      5       0  \n",
       "2  far good deliveri quit slow time even though l...      4       0  \n",
       "3  face realli experi warehous shown return track...      1       1  \n",
       "4  edit star final refund return declar miss cour...      3       1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 5 rows of our dataset\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3040, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3515 documents in our dataset\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3040 entries, 0 to 3039\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   content       3040 non-null   object\n",
      " 1   content_stem  3040 non-null   object\n",
      " 2   score         3040 non-null   int64 \n",
      " 3   target        3040 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 95.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the data types\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.526316\n",
       "1    0.473684\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish our baseline score\n",
    "reviews['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the majority class is class 0, our **baseline score** is an accuracy of **0.61**. The baseline score will serve as a point of comparison when evaluating our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews['content_stem']\n",
    "y = reviews['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train test split so that we can train, score and tune our models' hyperparameters \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2432,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.526316\n",
       "1    0.473684\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.526316\n",
       "1    0.473684\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2432, 3483)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use count vectorizer to check how many unique words there are\n",
    "cvec = CountVectorizer(stop_words='english') \n",
    "cvec_df = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names_out())\n",
    "cvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2632 unique words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes in the actual y value and model predictions, \n",
    "# and prints out the confusion matrix and classification report\n",
    "# Dataset: Validation or test set\n",
    "\n",
    "def cmat(actual_y, predictions, dataset):\n",
    "    \n",
    "    # Create a classification report\n",
    "    print('Classification report for', dataset)\n",
    "    print(classification_report(actual_y, predictions))\n",
    "    print('')\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(actual_y, predictions)\n",
    "    cm_df = pd.DataFrame(cm, columns=['Predicted Positive Review','Predicted Negative Review'], index=['Actual Positive Review', 'Actual Negative Review'])\n",
    "    print('Confusion matrix for', dataset)\n",
    "    print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore both classical machine learning and deep learning for sentiment analysis. The production model will be selected based on accuracy and recall on the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Bag of Words (BoW) representation to extract features from the text. This will be done through vectorization, specifically the CountVectorizer and TF-IDF Vectorizer. The CountVectorizer simply tokenizes and counts the word occurrences in our corpus. While on the other hand, TF-IDF tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power.  \n",
    "\n",
    "After vectorizing, we will fit a Logistic Regression, Naive Bayes and Support Vector Machine on the training data and evaluate the models' performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'cvec__max_df': 0.9, 'cvec__max_features': 300, 'cvec__min_df': 2, 'lr__C': 0.1, 'lr__penalty': 'l2'}\n",
      "Best CV score:  0.839227889091277\n",
      "Training score: 0.8667763157894737\n",
      "Validation score: 0.8305921052631579\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       320\n",
      "           1       0.83      0.80      0.82       288\n",
      "\n",
      "    accuracy                           0.83       608\n",
      "   macro avg       0.83      0.83      0.83       608\n",
      "weighted avg       0.83      0.83      0.83       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        274                         46\n",
      "Actual Negative Review                         57                        231\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with Count Vectorizer and Logistic Regression\n",
    "pipe_cvec_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')), \n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_cvec_lr_params = {\n",
    "    'cvec__max_features': [300], #100,200\n",
    "    'cvec__min_df': [2,3], \n",
    "    'cvec__max_df': [.9,.95], \n",
    "#     'cvec__ngram_range':[(1,1),(1,2)],  \n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__C': [.01,.1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_cvec_lr = GridSearchCV(pipe_cvec_lr, # Objects to optimise\n",
    "                          param_grid = pipe_cvec_lr_params, # Hyperparameters for tuning\n",
    "                          cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_cvec_lr.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "cvec_lr_pred = gs_cvec_lr.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_cvec_lr.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_cvec_lr.best_score_)\n",
    "print('Training score:', gs_cvec_lr.score(X_train, y_train))\n",
    "print('Validation score:', gs_cvec_lr.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, cvec_lr_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'lr__C': 1, 'lr__penalty': 'l2', 'tvec__max_df': 0.9, 'tvec__max_features': 300, 'tvec__min_df': 2}\n",
      "Best CV score:  0.8396545908385618\n",
      "Training score: 0.8704769736842105\n",
      "Validation score: 0.8355263157894737\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       320\n",
      "           1       0.82      0.84      0.83       288\n",
      "\n",
      "    accuracy                           0.84       608\n",
      "   macro avg       0.84      0.84      0.84       608\n",
      "weighted avg       0.84      0.84      0.84       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        266                         54\n",
      "Actual Negative Review                         46                        242\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with TF-IDF and Logistic Regression\n",
    "pipe_tvec_lr = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_tvec_lr_params = {\n",
    "    'tvec__max_features': [300], #100,200\n",
    "    'tvec__min_df': [2,3], #2,3 \n",
    "    'tvec__max_df': [.9,.95], \n",
    "#     'tvec__ngram_range':[(1,1),(1,2)],  \n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__C': [.1, 1] #.1, .01\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_tvec_lr = GridSearchCV(pipe_tvec_lr, # Objects to optimise\n",
    "                          param_grid = pipe_tvec_lr_params, # Hyperparameters for tuning\n",
    "                          cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_tvec_lr.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "tvec_lr_pred = gs_tvec_lr.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_tvec_lr.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_tvec_lr.best_score_)\n",
    "print('Training score:', gs_tvec_lr.score(X_train, y_train))\n",
    "print('Validation score:', gs_tvec_lr.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, tvec_lr_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'cvec__max_df': 0.9, 'cvec__max_features': 500, 'cvec__min_df': 3}\n",
      "Best CV score:  0.8400593671996223\n",
      "Training score: 0.8560855263157895\n",
      "Validation score: 0.8388157894736842\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       320\n",
      "           1       0.83      0.84      0.83       288\n",
      "\n",
      "    accuracy                           0.84       608\n",
      "   macro avg       0.84      0.84      0.84       608\n",
      "weighted avg       0.84      0.84      0.84       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        269                         51\n",
      "Actual Negative Review                         47                        241\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with Count Vectorizer and Naive Bayes\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_cvec_nb_params = {\n",
    "    'cvec__max_features': [500], #200\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.9,.95], \n",
    "#     'cvec__ngram_range':[(1,1),(1,2)],  \n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_cvec_nb = GridSearchCV(pipe_cvec_nb, # Objects to optimise\n",
    "                          param_grid = pipe_cvec_nb_params, # Hyperparameters for tuning\n",
    "                          cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_cvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "cvec_nb_pred = gs_cvec_nb.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_cvec_nb.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_cvec_nb.best_score_)\n",
    "print('Training score:', gs_cvec_nb.score(X_train, y_train))\n",
    "print('Validation score:', gs_cvec_nb.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, cvec_nb_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'tvec__max_df': 0.9, 'tvec__max_features': 500, 'tvec__min_df': 2}\n",
      "Best CV score:  0.8380068137354112\n",
      "Training score: 0.8614309210526315\n",
      "Validation score: 0.8355263157894737\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       320\n",
      "           1       0.83      0.83      0.83       288\n",
      "\n",
      "    accuracy                           0.84       608\n",
      "   macro avg       0.84      0.84      0.84       608\n",
      "weighted avg       0.84      0.84      0.84       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        270                         50\n",
      "Actual Negative Review                         50                        238\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with TF-IDF and Naive Bayes\n",
    "pipe_tvec_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words='english')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_tvec_nb_params = {\n",
    "    'tvec__max_features': [500], #200\n",
    "    'tvec__min_df': [2,3], #\n",
    "    'tvec__max_df': [.9,.95], \n",
    "#     'tvec__ngram_range':[(1,1),(1,2)],  \n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_tvec_nb = GridSearchCV(pipe_tvec_nb, # Objects to optimise\n",
    "                        param_grid = pipe_tvec_nb_params, # Hyperparameters for tuning\n",
    "                        cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_tvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "tvec_nb_pred = gs_tvec_nb.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_tvec_nb.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_tvec_nb.best_score_)\n",
    "print('Training score:', gs_tvec_nb.score(X_train, y_train))\n",
    "print('Validation score:', gs_tvec_nb.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, tvec_nb_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [[0.88685368 0.11314632]\n",
      " [0.5161704  0.4838296 ]]\n"
     ]
    }
   ],
   "source": [
    "text = 'The product is strong and great!'\n",
    "probs = gs_tvec_nb.predict_proba([text, 'This product is bad and ugly, slow, hard'])  # predict probabilities for the text\n",
    "print('Predicted probabilities:', probs)  # print the predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer & SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'cvec__max_df': 0.9, 'cvec__max_features': 300, 'cvec__min_df': 2, 'svc__C': 0.1, 'svc__kernel': 'linear'}\n",
      "Best CV score:  0.8347028266882548\n",
      "Training score: 0.8795230263157895\n",
      "Validation score: 0.8338815789473685\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85       320\n",
      "           1       0.85      0.80      0.82       288\n",
      "\n",
      "    accuracy                           0.83       608\n",
      "   macro avg       0.83      0.83      0.83       608\n",
      "weighted avg       0.83      0.83      0.83       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        278                         42\n",
      "Actual Negative Review                         59                        229\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with Count Vectorizer and SVC\n",
    "pipe_cvec_svc = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('svc', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_cvec_svc_params = {\n",
    "    'cvec__max_features': [300], #200,500\n",
    "    'cvec__min_df': [2,3], \n",
    "    'cvec__max_df': [.9,.95], \n",
    "#     'cvec__ngram_range':[(1,1),(1,2)],  \n",
    "    'svc__kernel': ['linear'], #'poly', 'rbf'\n",
    "#     'svc__degree': [3],\n",
    "    'svc__C': [.1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_cvec_svc = GridSearchCV(pipe_cvec_svc, # Objects to optimise\n",
    "                        param_grid = pipe_cvec_svc_params, # Hyperparameters for tuning\n",
    "                        cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_cvec_svc.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "cvec_svc_pred = gs_cvec_svc.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_cvec_svc.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_cvec_svc.best_score_)\n",
    "print('Training score:', gs_cvec_svc.score(X_train, y_train))\n",
    "print('Validation score:', gs_cvec_svc.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, cvec_svc_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF & SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'svc__C': 0.1, 'svc__kernel': 'linear', 'tvec__max_df': 0.9, 'tvec__max_features': 800, 'tvec__min_df': 2}\n",
      "Best CV score:  0.8380068137354112\n",
      "Training score: 0.8630756578947368\n",
      "Validation score: 0.84375\n",
      "\n",
      "Classification report for validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.85       320\n",
      "           1       0.81      0.88      0.84       288\n",
      "\n",
      "    accuracy                           0.84       608\n",
      "   macro avg       0.85      0.85      0.84       608\n",
      "weighted avg       0.85      0.84      0.84       608\n",
      "\n",
      "\n",
      "Confusion matrix for validation set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        259                         61\n",
      "Actual Negative Review                         34                        254\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with TF-IDF Vectorizer and SVC\n",
    "pipe_tvec_svc = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words='english')),\n",
    "    ('svc', SVC(probability=True, random_state=42)) \n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "pipe_tvec_svc_params = {\n",
    "    'tvec__max_features': [800], #200,500\n",
    "    'tvec__min_df': [2,3], \n",
    "    'tvec__max_df': [.9,.95], \n",
    "#     'tvec__ngram_range':[(1,1),(1,2)],  \n",
    "    'svc__kernel': ['linear'], #'poly', 'rbf'\n",
    "#     'svc__degree': [3],\n",
    "    'svc__C': [.1] # .01\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_tvec_svc = GridSearchCV(pipe_tvec_svc, # Objects to optimise\n",
    "                          param_grid = pipe_tvec_svc_params, # Hyperparameters for tuning\n",
    "                          cv=10) # 10-fold cross validation\n",
    "\n",
    "# Fit model on to training data\n",
    "gs_tvec_svc.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "tvec_svc_pred = gs_tvec_svc.predict(X_val)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters: ', gs_tvec_svc.best_params_)\n",
    "\n",
    "# Print accuracy scores\n",
    "print('Best CV score: ', gs_tvec_svc.best_score_)\n",
    "print('Training score:', gs_tvec_svc.score(X_train, y_train))\n",
    "print('Validation score:', gs_tvec_svc.score(X_val, y_val))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_val, tvec_svc_pred, 'validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Production Model on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will evaluate our model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test set into a dataframe\n",
    "test = pd.read_csv('../SentimentModel/clean_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 870 documents in our test set\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.526316\n",
       "1    0.473684\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The class representation in our test set looks similar to our training set as we used stratify\n",
    "test['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish our X and y variables\n",
    "X_test = test['content_stem']\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "test_pred = gs_tvec_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for test set\n",
      "\n",
      "Accuracy score:  0.8592105263157894\n",
      "\n",
      "Classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       400\n",
      "           1       0.85      0.85      0.85       360\n",
      "\n",
      "    accuracy                           0.86       760\n",
      "   macro avg       0.86      0.86      0.86       760\n",
      "weighted avg       0.86      0.86      0.86       760\n",
      "\n",
      "\n",
      "Confusion matrix for test set\n",
      "                        Predicted Positive Review  Predicted Negative Review\n",
      "Actual Positive Review                        347                         53\n",
      "Actual Negative Review                         54                        306\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation metrics for test set')\n",
    "print('')\n",
    "print('Accuracy score: ', accuracy_score(y_test, test_pred))\n",
    "print('')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "cmat(y_test, test_pred, 'test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our production model achieves 0.892 on accuracy and 0.87 on recall, we can conclude that the model generalises well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
